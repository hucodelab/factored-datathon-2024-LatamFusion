{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09422ecc-e935-4d11-96cf-85e48f9f94b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d526cfd-06c5-439e-b434-990e59e8b15f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read from bronze (landing) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9d333e-1744-4c33-af66-73219c06aa89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# account for landing files from https\n",
    "storage_account_name = \"factoredatathon\"\n",
    "storage_account_key = dbutils.secrets.get(scope=\"events\", key=\"landingBlobKey\")\n",
    "container_name = \"bronze\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\",\n",
    "    f\"{storage_account_key}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f066576-9fe8-4b34-bb6c-954e5aaf9774",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read from bronze\n",
    "file_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/*.CSV\"\n",
    "df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(file_path)\n",
    "\n",
    "columns = [\n",
    "    \"GLOBALEVENTID\", \"SQLDATE\", \"MonthYear\", \"Year\", \"FractionDate\",\n",
    "    \"Actor1Code\", \"Actor1Name\", \"Actor1CountryCode\", \"Actor1KnownGroupCode\",\n",
    "    \"Actor1EthnicCode\", \"Actor1Religion1Code\", \"Actor1Religion2Code\",\n",
    "    \"Actor1Type1Code\", \"Actor1Type2Code\", \"Actor1Type3Code\", \"Actor2Code\",\n",
    "    \"Actor2Name\", \"Actor2CountryCode\", \"Actor2KnownGroupCode\",\n",
    "    \"Actor2EthnicCode\", \"Actor2Religion1Code\", \"Actor2Religion2Code\",\n",
    "    \"Actor2Type1Code\", \"Actor2Type2Code\", \"Actor2Type3Code\", \"IsRootEvent\",\n",
    "    \"EventCode\", \"EventBaseCode\", \"EventRootCode\", \"QuadClass\",\n",
    "    \"GoldsteinScale\", \"NumMentions\", \"NumSources\", \"NumArticles\", \"AvgTone\",\n",
    "    \"Actor1Geo_Type\", \"Actor1Geo_FullName\", \"Actor1Geo_CountryCode\",\n",
    "    \"Actor1Geo_ADM1Code\", \"Actor1Geo_Lat\", \"Actor1Geo_Long\",\n",
    "    \"Actor1Geo_FeatureID\", \"Actor2Geo_Type\", \"Actor2Geo_FullName\",\n",
    "    \"Actor2Geo_CountryCode\", \"Actor2Geo_ADM1Code\", \"Actor2Geo_Lat\",\n",
    "    \"Actor2Geo_Long\", \"Actor2Geo_FeatureID\", \"ActionGeo_Type\",\n",
    "    \"ActionGeo_FullName\", \"ActionGeo_CountryCode\", \"ActionGeo_ADM1Code\",\n",
    "    \"ActionGeo_Lat\", \"ActionGeo_Long\", \"ActionGeo_FeatureID\", \"DATEADDED\",\n",
    "    \"SOURCEURL\"\n",
    "]\n",
    "\n",
    "df = df.toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8653c2cb-232f-485a-8ae8-0e30846b7711",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter df to 50 countries of the database\n",
    "\n",
    "server = \"factoredata2024.database.windows.net\"\n",
    "db = \"dactoredata2024\"\n",
    "user = \"factoredata2024admin\"\n",
    "password = dbutils.secrets.get(scope=\"events\", key=\"ASQLPassword\")\n",
    "\n",
    "# JDBC connection properties\n",
    "jdbc_url = f\"jdbc:sqlserver://{server}:1433;database={db};user={user}@{db};password={password};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": f\"{user}@{server}\",\n",
    "    \"password\": password,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Table name in Azure SQL Database\n",
    "table_name = \"[gkg].[50countries]\"\n",
    "\n",
    "# Write DataFrame to Azure SQL Database\n",
    "# count_df.write.jdbc(url=jdbc_url, table=table_name, mode=\"overwrite\", properties=connection_properties)\n",
    "\n",
    "# Read data from Azure SQL Database into DataFrame\n",
    "countries50 = spark.read \\\n",
    "    .jdbc(url=jdbc_url, table=table_name, properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "276fe575-a70f-489e-9443-a49fe3975dae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Collect the column from the smaller DataFrame as a list\n",
    "filter_values = countries50.select('countryCode').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Filter the large DataFrame using the collected list\n",
    "df = df \\\n",
    "    .filter(df['ActionGeo_CountryCode'].isin(filter_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e6e9735-34c6-4760-a590-e8d60b189488",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we write into silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62b230e-7560-46ea-bbf9-2fe2b95700d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'YYYYMMDD' string to a date format\n",
    "df = df.withColumn(\"DATE\", to_date(col(\"SQLDATE\"), \"yyyyMMdd\"))\n",
    "# Filter rows where the date is greater than '2023-08-13'\n",
    "df = df.filter(col(\"DATE\") > '2023-08-13')\n",
    "\n",
    "# Define the path where you want to save the Delta file in DBFS\n",
    "delta_path = \"/mnt/silver/eventsSilver\"\n",
    "# Write the DataFrame as a Delta file\n",
    "df = df.repartition(32)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "events_bronzeToSilver_PRD",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
