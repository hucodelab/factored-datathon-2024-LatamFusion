{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09422ecc-e935-4d11-96cf-85e48f9f94b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d526cfd-06c5-439e-b434-990e59e8b15f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read from bronze (landing) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9d333e-1744-4c33-af66-73219c06aa89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Account for landing files from https\n",
    "storage_account_name = \"factoredatathon\"\n",
    "storage_account_key = \"yDTqsi+AifQJPvC5r7L5iFFdmmj+fbxWr280etWbWMPXWij0yfmiuLJH3sZ91TI7SwmfR1SBD8L7+AStGVUo3Q==\"\n",
    "container_name = \"bronze\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\",\n",
    "    f\"{storage_account_key}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f066576-9fe8-4b34-bb6c-954e5aaf9774",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from bronze\n",
    "file_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/*.CSV\"\n",
    "df = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(file_path)\n",
    "\n",
    "# Adding column names\n",
    "columns = [\n",
    "    \"GLOBALEVENTID\", \"SQLDATE\", \"MonthYear\", \"Year\", \"FractionDate\",\n",
    "    \"Actor1Code\", \"Actor1Name\", \"Actor1CountryCode\", \"Actor1KnownGroupCode\",\n",
    "    \"Actor1EthnicCode\", \"Actor1Religion1Code\", \"Actor1Religion2Code\",\n",
    "    \"Actor1Type1Code\", \"Actor1Type2Code\", \"Actor1Type3Code\", \"Actor2Code\",\n",
    "    \"Actor2Name\", \"Actor2CountryCode\", \"Actor2KnownGroupCode\",\n",
    "    \"Actor2EthnicCode\", \"Actor2Religion1Code\", \"Actor2Religion2Code\",\n",
    "    \"Actor2Type1Code\", \"Actor2Type2Code\", \"Actor2Type3Code\", \"IsRootEvent\",\n",
    "    \"EventCode\", \"EventBaseCode\", \"EventRootCode\", \"QuadClass\",\n",
    "    \"GoldsteinScale\", \"NumMentions\", \"NumSources\", \"NumArticles\", \"AvgTone\",\n",
    "    \"Actor1Geo_Type\", \"Actor1Geo_FullName\", \"Actor1Geo_CountryCode\",\n",
    "    \"Actor1Geo_ADM1Code\", \"Actor1Geo_Lat\", \"Actor1Geo_Long\",\n",
    "    \"Actor1Geo_FeatureID\", \"Actor2Geo_Type\", \"Actor2Geo_FullName\",\n",
    "    \"Actor2Geo_CountryCode\", \"Actor2Geo_ADM1Code\", \"Actor2Geo_Lat\",\n",
    "    \"Actor2Geo_Long\", \"Actor2Geo_FeatureID\", \"ActionGeo_Type\",\n",
    "    \"ActionGeo_FullName\", \"ActionGeo_CountryCode\", \"ActionGeo_ADM1Code\",\n",
    "    \"ActionGeo_Lat\", \"ActionGeo_Long\", \"ActionGeo_FeatureID\", \"DATEADDED\",\n",
    "    \"SOURCEURL\"\n",
    "]\n",
    "\n",
    "df = df.toDF(*columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e6e9735-34c6-4760-a590-e8d60b189488",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Here we write into silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62b230e-7560-46ea-bbf9-2fe2b95700d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'YYYYMMDD' string to a date format\n",
    "df = df.withColumn(\"DATE\", to_date(col(\"SQLDATE\"), \"yyyyMMdd\"))\n",
    "# Filter rows where the date is greater than '2023-08-13'\n",
    "df = df.filter(col(\"DATE\") > '2023-08-13')\n",
    "\n",
    "# Define the path where you want to save the Delta file in DBFS\n",
    "delta_path = \"/mnt/silver/eventsSilver\"\n",
    "# Write the DataFrame as a Delta file\n",
    "df = df.repartition(32)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "events_bronzeToSilver",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
