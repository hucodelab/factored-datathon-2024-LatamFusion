{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the webpage containing the .csv files\n",
    "url = 'http://data.gdeltproject.org/events/index.html'  # Replace with the actual URL\n",
    "base_url = 'http://data.gdeltproject.org/events/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "files = soup.find_all('a', href=True)\n",
    "# # Example of parsing the webpage to find the links, sizes, and MD5s\n",
    "# files = soup.find_all('a', href=True)\n",
    "\n",
    "file_lists = []\n",
    "for file in files:\n",
    "    # print(file['href'])\n",
    "    if file['href'].endswith('.zip'):\n",
    "        file_lists.append(base_url+file['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 20240811.export.CSV to Azure Data Lake Storage\n",
      "Uploaded 20240812.export.CSV to Azure Data Lake Storage\n",
      "Uploaded 20240809.export.CSV to Azure Data Lake Storage\n",
      "Uploaded 20240813.export.CSV to Azure Data Lake Storage\n",
      "Uploaded 20240810.export.CSV to Azure Data Lake Storage\n"
     ]
    }
   ],
   "source": [
    "# Azure Storage account details\n",
    "import io\n",
    "\n",
    "storage_account_name = \"factoredatathon\"\n",
    "storage_account_key = \"yDTqsi+AifQJPvC5r7L5iFFdmmj+fbxWr280etWbWMPXWij0yfmiuLJH3sZ91TI7SwmfR1SBD8L7+AStGVUo3Q==\"\n",
    "container_name = \"bronze\"\n",
    "\n",
    "# Create a BlobServiceClient\n",
    "blob_service_client = BlobServiceClient(\n",
    "    f\"https://{storage_account_name}.blob.core.windows.net\", storage_account_key\n",
    ")\n",
    "\n",
    "# Function to download and upload a file\n",
    "def download_and_upload(url):\n",
    "    try:\n",
    "        # Download the file\n",
    "        file_name = os.path.basename(url)\n",
    "        response = requests.get(url)\n",
    "        local_path = f\"/tmp/{file_name}\"\n",
    "        \n",
    "        # Decompress the .zip file in memory\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            for zip_info in z.infolist():\n",
    "                # Extract the file to a byte stream\n",
    "                with z.open(zip_info) as extracted_file:\n",
    "                    # Upload the extracted file to Azure Data Lake Storage\n",
    "                    blob_name = zip_info.filename  # Use the name of the file inside the .zip\n",
    "                    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "                    blob_client.upload_blob(extracted_file.read(), overwrite=True)\n",
    "                    print(f\"Uploaded {blob_name} to Azure Data Lake Storage\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {file_name}: {e}\")\n",
    "        \n",
    "# Use ThreadPoolExecutor to download and upload files concurrently\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(download_and_upload, file_lists[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Uploaded 20240812.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240812.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240811.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240811.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240810.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240810.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240809.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240809.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240808.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240808.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240807.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240807.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240806.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240806.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240805.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240805.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240804.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240804.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240803.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240803.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240802.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240802.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240801.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240801.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240731.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240731.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240730.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240730.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240729.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240729.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240728.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240728.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240727.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240727.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240726.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240726.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240725.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240725.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240724.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240724.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240723.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240723.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240722.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240722.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240721.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240721.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240720.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240720.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240719.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240719.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240718.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240718.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240717.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240717.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240716.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240716.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240715.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240715.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240714.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240714.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240713.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240713.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240712.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240712.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240711.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240711.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240710.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240710.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240709.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240709.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240708.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240708.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240707.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240707.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240706.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240706.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240705.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240705.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240704.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240704.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240703.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240703.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240702.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240702.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240701.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240701.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240630.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240630.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240629.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240629.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240628.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240628.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240627.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240627.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240626.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240626.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240625.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240625.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240624.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240624.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240623.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240623.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240622.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240622.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240621.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240621.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240620.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240620.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240619.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240619.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240618.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240618.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240617.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240617.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240616.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240616.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240615.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240615.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240614.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240614.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240613.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240613.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240612.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240612.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240611.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240611.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240610.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240610.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240609.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240609.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240608.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240608.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240607.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240607.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240606.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240606.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240605.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240605.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240604.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240604.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240603.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240603.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240602.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240602.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240601.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240601.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240531.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240531.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240530.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240530.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240529.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240529.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240528.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240528.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240527.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240527.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240526.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240526.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240525.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240525.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240524.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240524.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240523.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240523.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240522.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240522.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240521.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240521.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240520.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240520.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240519.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240519.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240518.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240518.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240517.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240517.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240516.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240516.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240515.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240515.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240514.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240514.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240513.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240513.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240512.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240512.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240511.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240511.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240510.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240510.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240509.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240509.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240508.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240508.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240507.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240507.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240506.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240506.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240505.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240505.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240504.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240504.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240503.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240503.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240502.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240502.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240501.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240501.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240430.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240430.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240429.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240429.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240428.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240428.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240427.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240427.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240426.gkg.csv.zip to Azure Data Lake Storage\n",
      "1\n",
      "Uploaded 20240426.gkgcounts.csv.zip to Azure Data Lake Storage\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m count \u001b[39m=\u001b[39m\u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(count)\n\u001b[0;32m----> 5\u001b[0m download_and_upload(file)\n",
      "Cell \u001b[0;32mIn[43], line 16\u001b[0m, in \u001b[0;36mdownload_and_upload\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[39m# Download the file\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     file_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(url)\n\u001b[0;32m---> 16\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[1;32m     17\u001b[0m     local_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/tmp/\u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(local_path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     r\u001b[39m.\u001b[39;49mcontent\n\u001b[1;32m    748\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m   1062\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[39m=\u001b[39m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt, read1\u001b[39m=\u001b[39;49mread1) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread1(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for file in file_lists:\n",
    "    count =+ 1\n",
    "    print(count)\n",
    "    download_and_upload(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://data.gdeltproject.org/gkg/20240812.gkg.csv.zip',\n",
       " 'http://data.gdeltproject.org/gkg/20240812.gkgcounts.csv.zip',\n",
       " 'http://data.gdeltproject.org/gkg/20240811.gkg.csv.zip',\n",
       " 'http://data.gdeltproject.org/gkg/20240811.gkgcounts.csv.zip',\n",
       " 'http://data.gdeltproject.org/gkg/20240810.gkg.csv.zip',\n",
       " 'http://data.gdeltproject.org/gkg/20240810.gkgcounts.csv.zip',\n",
       " 'http://data.gdeltproject.org/gkg/20240809.gkg.csv.zip',\n",
       " 'http://data.gdeltproject.org/gkg/20240809.gkgcounts.csv.zip',\n",
       " 'http://data.gdeltproject.org/gkg/20240808.gkg.csv.zip',\n",
       " 'http://data.gdeltproject.org/gkg/20240808.gkgcounts.csv.zip']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_lists[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape the webpage and download files\n",
    "def scrape_and_download(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Example of parsing the webpage to find the links, sizes, and MD5s\n",
    "    files = soup.find_all('a', href=True)\n",
    "    for file in files:\n",
    "        if file['href'].endswith('.csv.zip'):\n",
    "            file_url = file['href']\n",
    "            file_name = os.path.basename(file_url)\n",
    "            save_path = os.path.join(os.getcwd(), file_name)\n",
    "            \n",
    "            # Extract the corresponding file size and MD5 (pseudo-code, depends on the actual HTML structure)\n",
    "            file_size = 'size'  # Replace with actual extraction logic\n",
    "            md5sum = 'md5'      # Replace with actual extraction logic\n",
    "\n",
    "            # Download the file\n",
    "            download_file(file_url, save_path)\n",
    "            print(f'Downloaded: {file_name}')\n",
    "\n",
    "            # Verify MD5 checksum\n",
    "            downloaded_md5 = calculate_md5(save_path)\n",
    "            if downloaded_md5 == md5sum:\n",
    "                print(f'MD5 checksum verified for {file_name}')\n",
    "            else:\n",
    "                print(f'Warning: MD5 checksum mismatch for {file_name}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '20240812.gkg.csv.zip': No scheme supplied. Perhaps you meant https://20240812.gkg.csv.zip?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Execute the function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m scrape_and_download(url)\n",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m, in \u001b[0;36mscrape_and_download\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     16\u001b[0m md5sum \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmd5\u001b[39m\u001b[39m'\u001b[39m      \u001b[39m# Replace with actual extraction logic\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Download the file\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m download_file(file_url, save_path)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDownloaded: \u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[39m# Verify MD5 checksum\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(file_url, save_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_file\u001b[39m(file_url, save_path):\n\u001b[0;32m----> 6\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(file_url, stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      7\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(save_path, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m         \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39miter_content(\u001b[39m1024\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39m# Create the Request.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m req \u001b[39m=\u001b[39m Request(\n\u001b[1;32m    564\u001b[0m     method\u001b[39m=\u001b[39mmethod\u001b[39m.\u001b[39mupper(),\n\u001b[1;32m    565\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     hooks\u001b[39m=\u001b[39mhooks,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m prep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_request(req)\n\u001b[1;32m    577\u001b[0m proxies \u001b[39m=\u001b[39m proxies \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    579\u001b[0m settings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmerge_environment_settings(\n\u001b[1;32m    580\u001b[0m     prep\u001b[39m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[1;32m    581\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    481\u001b[0m     auth \u001b[39m=\u001b[39m get_netrc_auth(request\u001b[39m.\u001b[39murl)\n\u001b[1;32m    483\u001b[0m p \u001b[39m=\u001b[39m PreparedRequest()\n\u001b[0;32m--> 484\u001b[0m p\u001b[39m.\u001b[39;49mprepare(\n\u001b[1;32m    485\u001b[0m     method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(),\n\u001b[1;32m    486\u001b[0m     url\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl,\n\u001b[1;32m    487\u001b[0m     files\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mfiles,\n\u001b[1;32m    488\u001b[0m     data\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    489\u001b[0m     json\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mjson,\n\u001b[1;32m    490\u001b[0m     headers\u001b[39m=\u001b[39;49mmerge_setting(\n\u001b[1;32m    491\u001b[0m         request\u001b[39m.\u001b[39;49mheaders, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheaders, dict_class\u001b[39m=\u001b[39;49mCaseInsensitiveDict\n\u001b[1;32m    492\u001b[0m     ),\n\u001b[1;32m    493\u001b[0m     params\u001b[39m=\u001b[39;49mmerge_setting(request\u001b[39m.\u001b[39;49mparams, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams),\n\u001b[1;32m    494\u001b[0m     auth\u001b[39m=\u001b[39;49mmerge_setting(auth, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth),\n\u001b[1;32m    495\u001b[0m     cookies\u001b[39m=\u001b[39;49mmerged_cookies,\n\u001b[1;32m    496\u001b[0m     hooks\u001b[39m=\u001b[39;49mmerge_hooks(request\u001b[39m.\u001b[39;49mhooks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhooks),\n\u001b[1;32m    497\u001b[0m )\n\u001b[1;32m    498\u001b[0m \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/models.py:367\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_method(method)\n\u001b[0;32m--> 367\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_url(url, params)\n\u001b[1;32m    368\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_headers(headers)\n\u001b[1;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[0;32m~/Desktop/workspace/datafactored2024/venv/lib/python3.9/site-packages/requests/models.py:438\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39m*\u001b[39me\u001b[39m.\u001b[39margs)\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m scheme:\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mraise\u001b[39;00m MissingSchema(\n\u001b[1;32m    439\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No scheme supplied. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPerhaps you meant https://\u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m host:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidURL(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid URL \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m: No host supplied\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mMissingSchema\u001b[0m: Invalid URL '20240812.gkg.csv.zip': No scheme supplied. Perhaps you meant https://20240812.gkg.csv.zip?"
     ]
    }
   ],
   "source": [
    "# Execute the function\n",
    "scrape_and_download(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0747050815602162b654281df3a7bcb28b8cad5fe83bd7f26fef093538b4610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
